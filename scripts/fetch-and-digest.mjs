#!/usr/bin/env node
/**
 * ClawFeed é‡‡é›† + Digest ç”Ÿæˆè„šæœ¬
 *
 * ç”¨æ³•:
 *   node scripts/fetch-and-digest.mjs [--type 4h|daily|weekly|monthly] [--deep]
 *
 * æ”¯æŒçš„ Source ç±»å‹:
 *   rss / atom       â€” RSS / Atom è®¢é˜…
 *   hackernews       â€” Hacker News çƒ­é—¨å¸–
 *   reddit           â€” Subreddit çƒ­é—¨å¸–
 *   github_trending  â€” GitHub Trending
 *
 * éœ€è¦ .env ä¸­é…ç½®:
 *   API_KEY          â€” ClawFeed æœåŠ¡ API Key
 *   DEEPSEEK_API_KEY â€” SiliconFlow DeepSeek API Key
 *
 * --deep æ¨¡å¼: å¯¹ Digest ç²¾é€‰çš„æ¯ç¯‡æ–‡ç« æŠ“å–åŸæ–‡ï¼Œç”Ÿæˆ 250 å­—ä¸­æ–‡æ·±åº¦æ‘˜è¦
 */

import https from 'https';
import http from 'http';
import { readFileSync, existsSync } from 'fs';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';
import { ProxyAgent, fetch as undiciFetch } from 'undici';

const __dirname = dirname(fileURLToPath(import.meta.url));
const ROOT = join(__dirname, '..');

// â”€â”€ Load .env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const envPath = join(ROOT, '.env');
const env = {};
if (existsSync(envPath)) {
  for (const line of readFileSync(envPath, 'utf8').split('\n')) {
    const trimmed = line.trim();
    if (!trimmed || trimmed.startsWith('#')) continue;
    const eq = trimmed.indexOf('=');
    if (eq > 0) env[trimmed.slice(0, eq).trim()] = trimmed.slice(eq + 1).trim();
  }
}

const API_KEY = env.API_KEY || process.env.API_KEY || '';
const DEEPSEEK_API_KEY = env.DEEPSEEK_API_KEY || process.env.DEEPSEEK_API_KEY || '';
const PORT = parseInt(env.DIGEST_PORT || process.env.DIGEST_PORT || '8767', 10);
const PROXY_URL = env.HTTP_PROXY || env.HTTPS_PROXY || env.http_proxy || env.https_proxy
  || process.env.HTTP_PROXY || process.env.HTTPS_PROXY || process.env.http_proxy || process.env.https_proxy || '';
const FEISHU_WEBHOOK = env.FEISHU_WEBHOOK || process.env.FEISHU_WEBHOOK || '';
const FEISHU_SECRET = env.FEISHU_SECRET || process.env.FEISHU_SECRET || '';

// â”€â”€ CLI args â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const args = process.argv.slice(2);
const getArg = (flag) => {
  const eqIdx = args.findIndex(a => a.startsWith(`${flag}=`));
  if (eqIdx !== -1) return args[eqIdx].split('=').slice(1).join('=');
  const idx = args.indexOf(flag);
  return idx !== -1 ? args[idx + 1] : null;
};

const DIGEST_TYPE = getArg('--type') || '4h';
const DEEP_MODE = args.includes('--deep');
const VALID_TYPES = ['4h', 'daily', 'weekly', 'monthly'];
if (!VALID_TYPES.includes(DIGEST_TYPE)) {
  console.error(`é”™è¯¯: --type å¿…é¡»æ˜¯ ${VALID_TYPES.join(' | ')} ä¹‹ä¸€`);
  process.exit(1);
}

// â”€â”€ Logger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const log = (...a) => console.log(`[${new Date().toISOString().slice(0, 19).replace('T', ' ')}]`, ...a);
const warn = (...a) => console.warn(`[${new Date().toISOString().slice(0, 19).replace('T', ' ')}] âš ï¸`, ...a);

// â”€â”€ HTTP helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const FETCH_TIMEOUT = 15000;

// Build proxy dispatcher once (reuse across requests)
const proxyDispatcher = PROXY_URL ? new ProxyAgent(PROXY_URL) : null;

async function httpFetch(url, { headers = {}, timeout = FETCH_TIMEOUT, maxBytes = 600000 } = {}) {
  const fetchOpts = {
    headers: { 'User-Agent': 'ClawFeed-Fetcher/1.0', ...headers },
    signal: AbortSignal.timeout(timeout),
    redirect: 'follow',
  };
  if (proxyDispatcher) fetchOpts.dispatcher = proxyDispatcher;

  const res = await undiciFetch(url, fetchOpts);
  // Read body with size limit
  const reader = res.body.getReader();
  const chunks = [];
  let total = 0;
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    chunks.push(value);
    total += value.length;
    if (total > maxBytes) { reader.cancel(); break; }
  }
  const body = Buffer.concat(chunks.map(c => Buffer.from(c))).toString('utf8');
  return { status: res.status, body, headers: Object.fromEntries(res.headers) };
}

// POST JSON to any HTTPS URL (used for Feishu webhook)
async function postJson(url, body) {
  const payload = JSON.stringify(body);
  const fetchOpts = {
    method: 'POST',
    headers: { 'Content-Type': 'application/json', 'User-Agent': 'ClawFeed-Bot/1.0' },
    body: payload,
    signal: AbortSignal.timeout(10000),
  };
  if (proxyDispatcher) fetchOpts.dispatcher = proxyDispatcher;
  const res = await undiciFetch(url, fetchOpts);
  const reader = res.body.getReader();
  const chunks = [];
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    chunks.push(value);
  }
  return { status: res.status, body: Buffer.concat(chunks.map(c => Buffer.from(c))).toString('utf8') };
}

// â”€â”€ Feishu / Lark Webhook Push â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// é£ä¹¦è‡ªå®šä¹‰æœºå™¨äººç­¾åç®—æ³•ï¼šHMAC-SHA256(key = timestamp+"\n"+secret, message = "")ï¼Œbase64 ç¼–ç 
async function sendFeishuNotification(content) {
  if (!FEISHU_WEBHOOK) return;

  const { createHmac } = await import('crypto');

  // Truncate to 4000 chars to keep the message readable in group chat
  const text = content.length > 4000
    ? content.slice(0, 4000) + '\n\nâ€¦ï¼ˆå†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·è®¿é—® ClawFeedï¼‰'
    : content;

  const msgBody = { msg_type: 'text', content: { text } };

  if (FEISHU_SECRET) {
    const timestamp = Math.floor(Date.now() / 1000).toString();
    const stringToSign = `${timestamp}\n${FEISHU_SECRET}`;
    const sign = createHmac('sha256', stringToSign).update('').digest('base64');
    msgBody.timestamp = timestamp;
    msgBody.sign = sign;
  }

  try {
    const resp = await postJson(FEISHU_WEBHOOK, msgBody);
    const result = JSON.parse(resp.body);
    if (result.code === 0 || result.StatusCode === 0) {
      log('âœ… é£ä¹¦æ¨é€æˆåŠŸ');
    } else {
      warn(`é£ä¹¦æ¨é€å¤±è´¥ (code=${result.code ?? result.StatusCode}): ${result.msg || result.StatusMessage || JSON.stringify(result)}`);
    }
  } catch (e) {
    warn(`é£ä¹¦æ¨é€å¼‚å¸¸: ${e.message}`);
  }
}

// POST to local ClawFeed API
function localPost(path, data, extraHeaders = {}) {
  const payload = JSON.stringify(data);
  return new Promise((resolve, reject) => {
    const req = http.request({
      hostname: '127.0.0.1', port: PORT, path,
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Content-Length': Buffer.byteLength(payload),
        ...extraHeaders,
      },
    }, (res) => {
      let body = '';
      res.on('data', c => body += c);
      res.on('end', () => {
        try { resolve({ status: res.statusCode, data: JSON.parse(body) }); }
        catch { resolve({ status: res.statusCode, data: body }); }
      });
    });
    req.setTimeout(10000, () => { req.destroy(); reject(new Error('Local API timeout')); });
    req.on('error', reject);
    req.write(payload);
    req.end();
  });
}

// â”€â”€ RSS / Atom Parser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function stripHtml(s) {
  return (s || '').replace(/<[^>]+>/g, ' ').replace(/&amp;/g, '&').replace(/&lt;/g, '<').replace(/&gt;/g, '>').replace(/&quot;/g, '"').replace(/&#39;/g, "'").replace(/\s+/g, ' ').trim();
}

function xmlText(block, tag) {
  const m = block.match(new RegExp(`<${tag}(?:[^>]*)>(?:<!\\[CDATA\\[)?([\\s\\S]*?)(?:\\]\\]>)?<\\/${tag}>`, 'i'));
  return m ? stripHtml(m[1].trim()) : '';
}

function xmlAttr(block, tag, attr) {
  const m = block.match(new RegExp(`<${tag}[^>]*\\s${attr}=["']([^"']+)["']`, 'i'));
  return m ? m[1].trim() : '';
}

async function fetchRss(url, limit = 20) {
  const { body } = await httpFetch(url);
  const items = [];
  const re = /<item[^>]*>([\s\S]*?)<\/item>|<entry[^>]*>([\s\S]*?)<\/entry>/gi;
  let m;
  while ((m = re.exec(body)) && items.length < limit) {
    const block = m[1] || m[2];
    const title = xmlText(block, 'title');
    const link =
      xmlText(block, 'link') ||
      xmlAttr(block, 'link', 'href') ||
      xmlText(block, 'id');
    const description = (
      xmlText(block, 'content:encoded') ||
      xmlText(block, 'description') ||
      xmlText(block, 'summary') ||
      xmlText(block, 'content')
    ).slice(0, 400);
    const pubDate = xmlText(block, 'pubDate') || xmlText(block, 'published') || xmlText(block, 'updated');
    const author = xmlText(block, 'author') || xmlText(block, 'dc:creator') || xmlText(block, 'name');
    if (!title && !link) continue;
    items.push({ title, url: link, description, pubDate, author });
  }
  return items;
}

// â”€â”€ Hacker News â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function fetchHackerNews({ filter = 'top', min_score = 50, limit = 20 } = {}) {
  const typeMap = { top: 'topstories', new: 'newstories', best: 'beststories', ask: 'askstories', show: 'showstories' };
  const listType = typeMap[filter] || 'topstories';
  const { body } = await httpFetch(`https://hacker-news.firebaseio.com/v2/${listType}.json`, { timeout: 8000 });
  const parsed = JSON.parse(body);
  const ids = (Array.isArray(parsed) ? parsed : []).slice(0, Math.min(limit * 3, 60));

  const results = await Promise.allSettled(
    ids.map(id => httpFetch(`https://hacker-news.firebaseio.com/v2/item/${id}.json`, { timeout: 5000 }).then(r => JSON.parse(r.body)))
  );

  return results
    .filter(r => r.status === 'fulfilled' && r.value?.title)
    .map(r => r.value)
    .filter(s => (s.score || 0) >= (min_score || 0))
    .slice(0, limit)
    .map(s => ({
      title: s.title,
      url: s.url || `https://news.ycombinator.com/item?id=${s.id}`,
      description: s.text
        ? stripHtml(s.text).slice(0, 300)
        : `${s.score} åˆ† Â· ${s.descendants || 0} è¯„è®º`,
      author: s.by,
    }));
}

// â”€â”€ Reddit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function fetchReddit({ subreddit, sort = 'hot', limit = 20 } = {}) {
  const { body } = await httpFetch(
    `https://www.reddit.com/r/${subreddit}/${sort}.json?limit=${limit}&raw_json=1`,
    { headers: { 'User-Agent': 'ClawFeed/1.0 (news aggregator bot)' }, timeout: 10000 }
  );
  const data = JSON.parse(body);
  return (data.data?.children || [])
    .map(c => c.data)
    .filter(p => p.title)
    .slice(0, limit)
    .map(p => ({
      title: p.title,
      url: p.url?.startsWith('/r/') ? `https://www.reddit.com${p.url}` : (p.url || `https://www.reddit.com${p.permalink}`),
      description: p.selftext
        ? p.selftext.slice(0, 300)
        : `â†‘${p.score} Â· ${p.num_comments} è¯„è®º Â· r/${p.subreddit}`,
      author: p.author,
    }));
}

// â”€â”€ GitHub Trending â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function fetchGitHubTrending({ language = '', since = 'daily' } = {}) {
  const langPath = language && language !== 'all' ? `/${encodeURIComponent(language)}` : '';
  const { body } = await httpFetch(`https://github.com/trending${langPath}?since=${since}`, { timeout: 12000 });

  const items = [];
  // Match each repo article block
  const repoRe = /href="\/([A-Za-z0-9_.-]+\/[A-Za-z0-9_.-]+)"\s*>/g;
  const seen = new Set();
  let m;
  while ((m = repoRe.exec(body)) && items.length < 25) {
    const repo = m[1];
    if (seen.has(repo) || repo.includes('/login') || repo.includes('/trending')) continue;
    seen.add(repo);

    // Try to find description near this position
    const excerpt = body.slice(m.index, m.index + 600);
    const descM = excerpt.match(/<p[^>]*>\s*([^<]{10,200})\s*<\/p>/);
    const starsM = body.slice(m.index, m.index + 1200).match(/(\d[\d,]*)\s*stars today/i);
    const desc = descM ? descM[1].trim() : '';
    const stars = starsM ? starsM[1].replace(/,/g, '') : '';

    items.push({
      title: repo,
      url: `https://github.com/${repo}`,
      description: [desc, stars ? `â­ ${stars} stars today` : ''].filter(Boolean).join(' Â· '),
    });
  }
  return items.slice(0, 20);
}

// â”€â”€ Dispatcher â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function fetchSource(source) {
  let config;
  try {
    config = typeof source.config === 'string' ? JSON.parse(source.config) : (source.config || {});
  } catch {
    config = {};
  }

  switch (source.type) {
    case 'rss':
    case 'atom':
    case 'digest_feed':
      return fetchRss(config.url);

    case 'hackernews':
      return fetchHackerNews(config);

    case 'reddit':
      return fetchReddit(config);

    case 'github_trending':
      return fetchGitHubTrending(config);

    default:
      warn(`æš‚ä¸æ”¯æŒçš„ Source ç±»å‹: ${source.type} (${source.name})ï¼Œå·²è·³è¿‡`);
      return [];
  }
}

// â”€â”€ Load sources from ClawFeed DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Import better-sqlite3 to read directly from DB (avoids auth complexity)
async function loadSources() {
  try {
    const { default: Database } = await import('better-sqlite3');
    const dbPath = process.env.DIGEST_DB || env.DIGEST_DB || join(ROOT, 'data', 'digest.db');
    if (!existsSync(dbPath)) {
      log('æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆå¯åŠ¨ ClawFeed æœåŠ¡å™¨åˆå§‹åŒ–æ•°æ®åº“');
      return [];
    }
    const db = new Database(dbPath, { readonly: true });
    const sources = db.prepare(
      'SELECT id, name, type, config, is_active, is_public FROM sources WHERE is_active = 1 AND is_deleted = 0'
    ).all();
    db.close();
    return sources;
  } catch (e) {
    warn('æ— æ³•è¯»å–æ•°æ®åº“ï¼Œå°è¯•é€šè¿‡ HTTP API è·å–:', e.message);
    // Fallback: HTTP API (only returns public sources without auth)
    const res = await httpFetch(`http://127.0.0.1:${PORT}/api/sources`);
    const sources = JSON.parse(res.body);
    return Array.isArray(sources) ? sources.filter(s => s.is_active && !s.is_deleted) : [];
  }
}

// â”€â”€ DeepSeek Digest Generator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function callDeepSeek(messages, maxTokens = 4096) {
  const payload = JSON.stringify({
    model: 'deepseek-ai/DeepSeek-V3',
    messages,
    temperature: 0.7,
    max_tokens: maxTokens,
  });

  return new Promise((resolve, reject) => {
    const req = https.request({
      hostname: 'api.siliconflow.cn',
      path: '/v1/chat/completions',
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${DEEPSEEK_API_KEY}`,
        'Content-Length': Buffer.byteLength(payload),
      },
    }, (res) => {
      let data = '';
      res.on('data', c => data += c);
      res.on('end', () => {
        try { resolve(JSON.parse(data)); }
        catch { reject(new Error(`DeepSeek å“åº”è§£æå¤±è´¥: ${data.slice(0, 300)}`)); }
      });
    });
    req.setTimeout(120000, () => { req.destroy(); reject(new Error('DeepSeek è¯·æ±‚è¶…æ—¶ï¼ˆ120sï¼‰')); });
    req.on('error', reject);
    req.write(payload);
    req.end();
  });
}

async function generateDigest(allItems, digestType) {
  const TYPE_NAMES = { '4h': '4å°æ—¶ç®€æŠ¥', daily: 'æ—¥æŠ¥', weekly: 'å‘¨æŠ¥', monthly: 'æœˆæŠ¥' };
  const now = new Date();
  const dateStr = now.toLocaleString('zh-CN', {
    timeZone: 'Asia/Shanghai',
    year: 'numeric', month: '2-digit', day: '2-digit',
    hour: '2-digit', minute: '2-digit',
  });

  // Format items as numbered list for the prompt
  const itemLines = allItems.map((item, i) => {
    const parts = [`${i + 1}. [${item._sourceName}] ${item.title || '(æ— æ ‡é¢˜)'}`];
    if (item.url) parts.push(`   é“¾æ¥: ${item.url}`);
    if (item.description) parts.push(`   ç®€ä»‹: ${item.description}`);
    return parts.join('\n');
  }).join('\n\n');

  const systemPrompt = `ä½ æ˜¯ä¸“ä¸š AI èµ„è®¯ç¼–è¾‘ï¼Œä»å¤šä¿¡æ¯æºä¸­ç²¾é€‰æœ€æœ‰ä»·å€¼å†…å®¹ï¼Œç”Ÿæˆç®€æ´æœ‰åŠ›çš„ä¸­æ–‡${TYPE_NAMES[digestType]}ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼Œä¸æ·»åŠ é¢å¤–è¯´æ˜ï¼‰ï¼š
â˜€ï¸ AI å¿«æŠ¥ | ${dateStr} CST

ğŸ”¥ é‡è¦åŠ¨æ€
â€¢ [æ ‡é¢˜] â€” ä¸€å¥è¯ç‚¹è¯„ [é“¾æ¥]
ï¼ˆä»… 2-4 æ¡çœŸæ­£é‡è¦çš„è¡Œä¸šæ–°é—»ï¼šå¤§é¢èèµ„ã€é‡å¤§äº§å“å‘å¸ƒã€çªç ´æ€§ç ”ç©¶ï¼‰

ğŸ“° ç²¾é€‰èµ„è®¯
â€¢ [å†…å®¹æ‘˜è¦] â€” ä¸ºä»€ä¹ˆå€¼å¾—çœ‹ [é“¾æ¥]
ï¼ˆ8-12 æ¡ï¼Œè¦†ç›–æŠ€æœ¯/äº§å“/è¡Œä¸šç­‰å¤šä¸ªç»´åº¦ï¼‰

ç¼–è¾‘è§„åˆ™ï¼š
- å…¨éƒ¨è¾“å‡ºä¸­æ–‡
- æ¯æ¡å¿…é¡»é™„ä¸ŠåŸå§‹é“¾æ¥
- å»é™¤å¹¿å‘Šã€è¥é”€å†…å®¹ã€é‡å¤æ¡ç›®
- ä¼˜å…ˆé€‰ä¿¡æ¯å¯†åº¦é«˜çš„åŸåˆ›å†…å®¹
- æ€»æ¡ç›®ä¸è¶…è¿‡ 15 æ¡`;

  const userPrompt = `ä»¥ä¸‹æ˜¯ä» ${[...new Set(allItems.map(i => i._sourceName))].join('ã€')} ç­‰ ${allItems.length} æ¡å†…å®¹ï¼Œè¯·ç”Ÿæˆ${TYPE_NAMES[digestType]}ï¼š\n\n${itemLines}`;

  const result = await callDeepSeek([
    { role: 'system', content: systemPrompt },
    { role: 'user', content: userPrompt },
  ]);

  const content = result.choices?.[0]?.message?.content?.trim();
  if (!content) throw new Error(result.error?.message || result.error?.msg || 'DeepSeek è¿”å›ç©ºå†…å®¹');
  return content;
}

// â”€â”€ Deep mode: article fetch + per-article summarization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Domains that don't contain readable article text
const SKIP_ARTICLE_DOMAINS = new Set([
  'reddit.com', 'v.redd.it', 'i.redd.it', 'old.reddit.com',
  'twitter.com', 'x.com', 't.co',
  'youtube.com', 'youtu.be',
  'github.com', 'gist.github.com',
  'news.ycombinator.com',
  'instagram.com', 'linkedin.com', 'facebook.com',
  'imgur.com', 'giphy.com',
]);

const shouldFetchArticle = (url) => {
  try {
    const host = new URL(url).hostname.replace(/^www\./, '');
    return !SKIP_ARTICLE_DOMAINS.has(host);
  } catch { return false; }
};

async function fetchArticleText(url, maxChars = 12000) {
  try {
    const { body } = await httpFetch(url, { timeout: 15000 });

    // Strip noise elements
    let html = body
      .replace(/<script[\s\S]*?<\/script>/gi, '')
      .replace(/<style[\s\S]*?<\/style>/gi, '')
      .replace(/<nav[\s\S]*?<\/nav>/gi, '')
      .replace(/<header[\s\S]*?<\/header>/gi, '')
      .replace(/<footer[\s\S]*?<\/footer>/gi, '')
      .replace(/<aside[\s\S]*?<\/aside>/gi, '')
      .replace(/<figure[\s\S]*?<\/figure>/gi, '')
      .replace(/<!--[\s\S]*?-->/g, '');

    // Find best content container in priority order
    let contentHtml = '';
    const selectors = [
      /<article[^>]*>([\s\S]*?)<\/article>/i,
      /<main[^>]*>([\s\S]*?)<\/main>/i,
      /<div[^>]*class="[^"]*(?:article-body|post-content|entry-content|story-body|article__body|post-body)[^"]*"[^>]*>([\s\S]*?)<\/div>/i,
      /<div[^>]*id="[^"]*(?:article|content|story|post)[^"]*"[^>]*>([\s\S]*?)<\/div>/i,
    ];
    for (const re of selectors) {
      const m = html.match(re);
      if (m && m[1].length > 400) { contentHtml = m[1]; break; }
    }
    if (!contentHtml) contentHtml = html;

    // Extract paragraph text
    const paragraphs = [];
    const pRe = /<p[^>]*>([\s\S]*?)<\/p>/gi;
    let m;
    while ((m = pRe.exec(contentHtml))) {
      const text = m[1].replace(/<[^>]+>/g, ' ')
        .replace(/&amp;/g, '&').replace(/&lt;/g, '<').replace(/&gt;/g, '>')
        .replace(/&quot;/g, '"').replace(/&#39;/g, "'").replace(/&nbsp;/g, ' ')
        .replace(/\s+/g, ' ').trim();
      if (text.length > 40) paragraphs.push(text);
    }

    if (paragraphs.length >= 3) return paragraphs.join('\n\n').slice(0, maxChars);

    // Fallback: strip all HTML
    return contentHtml.replace(/<[^>]+>/g, ' ').replace(/\s+/g, ' ').trim().slice(0, maxChars);
  } catch {
    return '';
  }
}

async function summarizeArticle(title, url, sourceName, articleText) {
  if (!articleText || articleText.trim().length < 150) return null;

  const result = await callDeepSeek([
    {
      role: 'system',
      content:
        'ä½ æ˜¯ä¸“ä¸šæ–‡ç« æ‘˜è¦ä¸“å®¶ã€‚å°†ç»™å®šçš„æ–‡ç« å†…å®¹æ€»ç»“æˆ250å­—å·¦å³çš„ä¸­æ–‡æ‘˜è¦ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n' +
        '**æ ¸å¿ƒè¦ç‚¹**: ï¼ˆ1-2å¥è¯æ¦‚æ‹¬æ–‡ç« æœ€é‡è¦çš„å†…å®¹ï¼‰\n\n' +
        '**å…³é”®ä¿¡æ¯**:\nâ€¢ ...\nâ€¢ ...\nâ€¢ ...\n\n' +
        '**ä»·å€¼/å½±å“**: ï¼ˆ1å¥è¯è¯´æ˜è¿™ç¯‡æ–‡ç« ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼‰\n\n' +
        'åªè¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€æˆ–åè®°ã€‚',
    },
    {
      role: 'user',
      content: `æ¥æº: ${sourceName}\næ ‡é¢˜: ${title}\né“¾æ¥: ${url}\n\næ–‡ç« å†…å®¹:\n${articleText}`,
    },
  ], 1024);

  return result.choices?.[0]?.message?.content?.trim() || null;
}

async function generateDeepSummaries(digestContent, allItems) {
  // Extract all URLs from the generated digest
  const rawUrls = [];
  const urlRe = /https?:\/\/[^\s\)\]"'<>]+/g;
  let m;
  while ((m = urlRe.exec(digestContent))) {
    rawUrls.push(m[0].replace(/[.,;:!?ï¼‰]+$/, ''));
  }
  const digestUrls = [...new Set(rawUrls)].filter(shouldFetchArticle);

  if (digestUrls.length === 0) {
    warn('æ·±åº¦æ¨¡å¼ï¼šæœªä» Digest ä¸­æå–åˆ°å¯åˆ†æçš„æ–‡ç« é“¾æ¥');
    return null;
  }

  log(`\nğŸ” æ·±åº¦æ¨¡å¼ï¼šæ‰¾åˆ° ${digestUrls.length} ç¯‡æ–‡ç« ï¼Œå¼€å§‹å¹¶å‘æŠ“å–åŸæ–‡...`);

  // Build URL â†’ item metadata map
  const urlToItem = new Map();
  for (const item of allItems) {
    if (item.url) urlToItem.set(item.url, item);
  }

  // Fetch all articles concurrently
  const fetchResults = await Promise.allSettled(
    digestUrls.map(url =>
      fetchArticleText(url).then(text => ({ url, text }))
    )
  );

  // Summarize sequentially (avoid rate limits)
  log('æ­£åœ¨é€ç¯‡ç”Ÿæˆæ·±åº¦æ‘˜è¦...');
  const summaries = [];
  for (const result of fetchResults) {
    if (result.status !== 'fulfilled') continue;
    const { url, text } = result.value;
    const item = urlToItem.get(url);
    const title = item?.title || url;
    const sourceName = item?._sourceName || new URL(url).hostname.replace(/^www\./, '');

    process.stdout.write(`  ğŸ“„ ${title.slice(0, 55).padEnd(55)} ... `);
    const summary = await summarizeArticle(title, url, sourceName, text);
    if (summary) {
      summaries.push({ title, url, sourceName, summary });
      console.log('âœ“');
    } else {
      console.log('âœ— æ— æ³•è·å–åŸæ–‡');
    }
  }

  if (summaries.length === 0) return null;

  const deepSection = [
    'â•'.repeat(50),
    '',
    'ğŸ“– æ·±åº¦æ‘˜è¦',
    `ï¼ˆå…± ${summaries.length} ç¯‡ï¼Œç”± DeepSeek æ ¹æ®åŸæ–‡ç”Ÿæˆï¼‰`,
    '',
    summaries.map((s, i) =>
      `### ${i + 1}. ${s.title}\n> **æ¥æº**: ${s.sourceName} Â· [åŸæ–‡é“¾æ¥](${s.url})\n\n${s.summary}`
    ).join('\n\n---\n\n'),
  ].join('\n');

  return deepSection;
}

// â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function main() {
  // Pre-flight checks
  if (!API_KEY) {
    console.error('âŒ è¯·åœ¨ .env ä¸­è®¾ç½® API_KEY');
    process.exit(1);
  }
  if (!DEEPSEEK_API_KEY) {
    console.error('âŒ è¯·åœ¨ .env ä¸­è®¾ç½® DEEPSEEK_API_KEY');
    process.exit(1);
  }

  log(`å¼€å§‹ç”Ÿæˆ ${DIGEST_TYPE} Digest...`);

  // 1. Load sources
  log('æ­£åœ¨åŠ è½½ä¿¡æ¯æº...');
  const sources = await loadSources();
  if (sources.length === 0) {
    log('âŒ æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„ä¿¡æ¯æºã€‚è¯·å…ˆåœ¨ Web ç•Œé¢æ·»åŠ  Sourceï¼ˆRSS/HN/Reddit ç­‰ï¼‰ã€‚');
    log(`   æ‰“å¼€æµè§ˆå™¨è®¿é—® http://127.0.0.1:${PORT}`);
    process.exit(0);
  }
  log(`æ‰¾åˆ° ${sources.length} ä¸ªæ´»è·ƒä¿¡æ¯æº: ${sources.map(s => s.name).join(', ')}`);

  // 2. Fetch content from each source
  log('\nå¼€å§‹é‡‡é›†å†…å®¹...');
  const allItems = [];
  for (const source of sources) {
    process.stdout.write(`  é‡‡é›†: ${source.name} (${source.type}) ... `);
    try {
      const items = await fetchSource(source);
      console.log(`âœ“ ${items.length} æ¡`);
      for (const item of items) {
        allItems.push({ ...item, _sourceName: source.name, _sourceType: source.type });
      }
    } catch (e) {
      console.log(`âœ— å¤±è´¥: ${e.message}`);
    }
  }

  if (allItems.length === 0) {
    log('âŒ æ‰€æœ‰ä¿¡æ¯æºé‡‡é›†å‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Source é…ç½®æˆ–ç½‘ç»œè¿æ¥ã€‚');
    process.exit(1);
  }
  log(`\nå…±é‡‡é›†åˆ° ${allItems.length} æ¡å†…å®¹`);

  // 3. Generate standard digest via DeepSeek
  log('\næ­£åœ¨è°ƒç”¨ DeepSeek ç”Ÿæˆæ‘˜è¦ï¼ˆå¯èƒ½éœ€è¦ 20-60 ç§’ï¼‰...');
  let content = await generateDigest(allItems, DIGEST_TYPE);
  log(`âœ“ æ‘˜è¦ç”Ÿæˆå®Œæˆï¼ˆ${content.length} å­—ï¼‰`);

  // 4. (Optional) Deep mode: fetch articles + per-article summaries
  if (DEEP_MODE) {
    log('\nå¯ç”¨æ·±åº¦æ¨¡å¼ï¼Œå¼€å§‹æŠ“å–åŸæ–‡ç”Ÿæˆæ·±åº¦æ‘˜è¦...');
    log('ï¼ˆæ¯ç¯‡æ–‡ç« çº¦éœ€ 5-15 ç§’ï¼Œå…¨ç¨‹éœ€ 2-5 åˆ†é’Ÿï¼‰');
    const deepSection = await generateDeepSummaries(content, allItems);
    if (deepSection) {
      content = content + '\n\n' + deepSection;
      log(`âœ“ æ·±åº¦æ‘˜è¦è¿½åŠ å®Œæˆï¼Œæ€»å†…å®¹ ${content.length} å­—`);
    }
  }

  // 5. POST digest to ClawFeed
  log('\næ­£åœ¨ä¿å­˜ Digest åˆ° ClawFeed...');
  const postRes = await localPost(
    '/api/digests',
    { type: DIGEST_TYPE, content },
    { Authorization: `Bearer ${API_KEY}` }
  );

  if (postRes.status === 201) {
    log(`âœ… Digest ä¿å­˜æˆåŠŸï¼id = ${postRes.data.id}`);
    log(`   æŸ¥çœ‹: http://127.0.0.1:${PORT}`);

    // Push to Feishu group bot
    if (FEISHU_WEBHOOK) {
      log('\næ­£åœ¨æ¨é€åˆ°é£ä¹¦ç¾¤æœºå™¨äºº...');
      await sendFeishuNotification(content);
    }
  } else {
    console.error('âŒ ä¿å­˜å¤±è´¥:', JSON.stringify(postRes));
    process.exit(1);
  }

  // Print preview
  const preview = content.split('\n').slice(0, 20).join('\n');
  console.log('\n' + 'â”€'.repeat(60));
  console.log(preview);
  if (content.split('\n').length > 20) console.log('...');
  console.log('â”€'.repeat(60));
}

main().catch(e => {
  console.error('âŒ è‡´å‘½é”™è¯¯:', e.message);
  process.exit(1);
});
